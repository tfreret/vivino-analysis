{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c16b81",
   "metadata": {},
   "source": [
    "# Vivino Wine Analysis\n",
    "\n",
    "## Project Overview\n",
    "This notebook contains the analysis for the Vivino wine dataset. The project is structured as follows:\n",
    "1. **Data Exploratory Analysis & Unsupervised Exploration**\n",
    "2. **Data preprocessing, preparation & train-val-test splits**\n",
    "3. **Baseline results with basic Linear & Ensemble Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731976f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('data/25-11-2025.csv')\n",
    "    print(\"Data loaded successfully!\")\n",
    "    display(df.head())\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the path 'data/25-11-2025.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a055447",
   "metadata": {},
   "source": [
    "## 1. Data Exploratory Analysis & Unsupervised Exploration\n",
    "In this section, we will explore the dataset to understand the distribution of variables, correlations, and potential outliers. We will also perform unsupervised exploration if applicable (e.g., clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Visualizations\n",
    "\n",
    "# 1. Distribution of the target variable 'rating'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['rating'], bins=20, kde=True)\n",
    "plt.title('Distribution of Wine Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 2. Price vs Rating\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='price', y='rating', alpha=0.5)\n",
    "plt.title('Price vs Rating')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Rating')\n",
    "plt.xscale('log') # Log scale for price as it can be skewed\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation Matrix (Numeric features)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec66eeb",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing, preparation & train-val-test splits\n",
    "Here we will prepare the data for modeling. This includes handling missing values, encoding categorical variables, scaling features, and splitting the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4300c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Separate target and features\n",
    "# We drop 'id' and 'name' as they are identifiers and not useful for prediction\n",
    "X = df.drop(['rating', 'id', 'name'], axis=1)\n",
    "y = df['rating']\n",
    "\n",
    "# Handle 'vintage' column which might contain 'N.V.' or other non-numeric values\n",
    "# We coerce errors to NaN, so 'N.V.' becomes NaN\n",
    "X['vintage'] = pd.to_numeric(X['vintage'], errors='coerce')\n",
    "\n",
    "# Define feature groups based on the schema\n",
    "numeric_features = ['vintage', 'price', 'acidity', 'intensity', 'sweetness', 'tannin']\n",
    "categorical_features = ['country', 'winery', 'grapes', 'flavor_rank1', 'flavor_rank2', 'flavor_rank3']\n",
    "\n",
    "# Create transformers\n",
    "# Numeric: Impute missing values with median, then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical: Impute missing with 'missing', then OneHotEncode\n",
    "# We use max_categories to handle high cardinality columns like winery and grapes\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False, max_categories=20))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f3f95",
   "metadata": {},
   "source": [
    "## 3. Baseline results with basic Linear & Ensemble Models\n",
    "We will establish baseline performance using simple models like Linear Regression and Ensemble methods (e.g., Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "# Calculate RMSE manually as 'squared' parameter might not be supported in this version\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Linear Regression RMSE:\", rmse_lr)\n",
    "print(\"Random Forest RMSE:\", rmse_rf)\n",
    "\n",
    "print(\"Linear Regression R2:\", r2_score(y_test, y_pred_lr))\n",
    "print(\"Random Forest R2:\", r2_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa92118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Model Performance\n",
    "\n",
    "# 1. Actual vs Predicted (Random Forest)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_rf, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Rating')\n",
    "plt.ylabel('Predicted Rating')\n",
    "plt.title('Random Forest: Actual vs Predicted Ratings')\n",
    "plt.show()\n",
    "\n",
    "# 2. Feature Importance (Random Forest)\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = []\n",
    "if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "else:\n",
    "    # Fallback for older sklearn versions or if get_feature_names_out is not available\n",
    "    # This is a simplified fallback and might not be perfect\n",
    "    feature_names = numeric_features + list(preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features))\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_imp_df = feature_imp_df.sort_values(by='Importance', ascending=False).head(20) # Top 20 features\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_imp_df, palette='viridis')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
